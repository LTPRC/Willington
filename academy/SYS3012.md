# SYS301 - Distributed Systems - 分布式系统

返回[Bulletin](./bulletin.md)

返回[SYS301 - Distributed Systems](./SYS301.md)

[TOC]

## 分布式事务<img src="./icons/alibaba.gif" /><img src="./icons/paypal.gif" /><img src="./icons/xiaohui.gif" /><img src="./icons/meituan.gif" /><img src="./icons/bytedance.gif" /><img src="./icons/luban.gif" /><img src="./icons/netease.gif" />

分布式事务是指服务器位于分布式系统不同节点之上的事务，用于保证分布式系统不同节点之间的数据一致性。其实就是将对单个库的事务的概念扩大到了对多个库的事务。

分布式事务处理的关键是必须有一种方法可以知道事务在任何地方所做的所有动作，提交或回滚事务的决定必须产生统一的结果（全部提交或全部回滚）。

## 分布式一致性<img src="./icons/netease.gif" /><img src="./icons/luban.gif" /><img src="./icons/bytedance.gif" />

### 强一致性/原子一致性/线性一致性

任何一次读都能读到某个数据的最近一次写的数据。

系统中的所有进程，看到的操作顺序，都和全局时钟下的顺序一致。

### 顺序一致性

任何一次读都能读到某个数据的最近一次写的数据。

系统的所有进程的顺序一致，而且是合理的。即不需要和全局时钟下的顺序一致，错的话一起错，对的话一起对。

### 弱一致性

数据更新后，如果能容忍后续的访问只能访问到部分或者全部访问不到。

### 最终一致性

一段时间后，节点间的数据会最终达到一致状态。

## XA规范<img src="./icons/netease.gif" /><img src="./icons/xiaohui.gif" /><img src="./icons/gupao.gif" /><img src="./icons/luban.gif" /><img src="./icons/bytedance.gif" />

分布式事务的实现有很多种，最具有代表性的是由Oracle Tuxedo系统提出的XA分布式事务协议。在XA协议中包含着两个角色：事务协调者和事务参与者。

XA协议包含两阶段提交(2PC)和三阶段提交(3PC)两种实现。

### 两阶段提交（2PC）<img src="./icons/netease.gif" /><img src="./icons/paypal.gif" /><img src="./icons/alibaba.gif" /><img src="./icons/xiaohui.gif" /><img src="./icons/gupao.gif" /><img src="./icons/luban.gif" /><img src="./icons/bytedance.gif" />

#### 为什么需要2PC

在分布式的情况下，1PC没有办法一次性地使得分布式系统中牵涉的不同实例之间达到原子的状态。而2PC(两阶段提交)保证原子性的方式是参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作。

#### 投票阶段

协调者首先将命令写入日志，然后给参与者发prepare命令。

参与者收到prepare命令后，根据自己的实际情况判断是否可以提交，将结果记录到日志系统，然后将结果返回给协调者。

<img src="./images/SYS301010.png" />

#### 决定阶段

当协调者收到所有参与者的确认消息后，判断所有协调者是否都可以提交。

- 如果可以，写入日志，发起commit命令。

- 如果有任意一个不可以，写入日志，发起abort命令。

参与者收到协调者发起的命令后执行，将执行命令及结果写入日志，返回结果给协调者。

<img src="./images/SYS301011.png" />

#### 存在的问题

##### 性能问题

整个消息链路是串行的，要等待响应结果，响应时间长，不适合高并发的场景。

##### 单点故障

事务协调者是整个XA模型的核心，协调者挂掉可以重新选举，但是无法解决因此处于阻塞状态的参与者的问题。

##### 丢失消息导致数据不一致

在XA协议的第二个阶段，如果发生局部网络问题，一部分事务参与者收到了提交消息，另一部分事务参与者没收到提交消息，那么就导致了节点之间数据的不一致。

##### 事务状态不确定

协调者在发出commit消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。

### 三阶段提交（3PC）<img src="./icons/netease.gif" /><img src="./icons/paypal.gif" /><img src="./icons/alibaba.gif" /><img src="./icons/xiaohui.gif" /><img src="./icons/gupao.gif" /><img src="./icons/luban.gif" /><img src="./icons/bytedance.gif" />

2PC中只有协调者可以超时，参与者没有超时机制。3PC增加了CanCommit阶段，引入了参与者超时机制，一旦迟迟没有接到协调者的commit请求，会自动进行本地commit。这样有效解决了协调者**单点故障**和**事务状态不确定**的问题，但是不能从根本上解决**性能问题**和**数据不一致**问题。

#### CanCommit阶段

协调者询问事务参与者，你是否有能力完成此次事务。

- 如果都返回yes，则进入第二阶段。

- 有一个返回no或等待响应超时，则中断事务，并向所有参与者发送abort请求。

#### PreCommit阶段

协调者向所有的参与者发送PreCommit请求。

参与者收到后开始执行事务操作，并将Undo和Redo信息记录到事务日志中。执行完事务操作后反馈“Ack”并等待下一步指令。

#### DoCommit阶段

如果所有的参与者节点都可以进行PreCommit提交，那么协调者就会从“预提交状态”转变为“提交状态”，然后向所有的参与者节点发送"doCommit"请求。

参与者节点在收到提交请求后就会各自执行事务提交操作，并向协调者节点反馈“Ack”消息。

协调者收到所有参与者的Ack消息后完成事务。

相反，如果有一个参与者节点未完成PreCommit的反馈或者反馈超时，那么协调者都会向所有的参与者节点发送abort请求，从而中断事务。

### Precolator<img src="./icons/bytedance.gif" />

Percolator是一个2PC的事务模型，主要通过lock保证事务的原子性，通过MVCC保证隔离性。通过对事务涉及的数据扩展两个列来实现lock和MVCC。通过下面Percolator paper中的数据模型图可以有一个直观的印象：

<img src="./images/SYS301012.png" />

其中data列是数据列；lock列为事务状态列；write列为commit标志列，其value有一个指向data列的时间戳来表达对应提交的版本。

Percolator是没有传统上的TransactionManager来管理事务的，将原本2PC中TransactionManager管理的集中化的事务状态信息分散在每一行的数据中（每个事务的primary row里），对于未决的情况通过lock的信息就能找到 primary row，进而就能确定这个事务的状态。

#### 准备阶段

Client先进入prepare阶段，先选出任意一个write作为primary执行prewrite动作，就是在data列写入数据，且在lock列写入状态信息。如果没有版本和状态冲突，则prewrite成功。

如果primary成功了则开始对secondaries(其他的)进行prewrite(可并行)，如果都prewrite成功了，则进入commit阶段，否则任何一个失败则回滚。这里需要注意的是secondary row的lock的value记录了primary row的信息，也就是说通过其lock能找到primary来进行状态决议。

#### 提交阶段

先对primary进行commit, 即去掉lock, 在write列写入一条记录，如果成功了则异步对secondaries进行commit, 如果primary commit失败了则回滚。

### TCC(Try-Confirm-Cancel)补偿事务<img src="./icons/bytedance.gif" /><img src="./icons/alibaba.gif" /><img src="./icons/xiaohui.gif" />

TCC事务是Try、Commit、Cancel三种指令的缩写，核心思想是“针对每个操作，都要注册一个与其对应的确认和补偿（撤销）操作”，其实就是2PC的一种特化实现，设计上讲究的是让业务做更多、业务可以做决策。不同的业务场景所写的代码都不一样，并且很大程度的增加了业务代码的复杂度，因此，这种模式并不能很好地被复用。

#### 对比2PC

##### 解决了协调者单点

引入多点的业务活动管理器，由主业务方发起并完成这个业务活动。

##### 同步阻塞

引入超时机制，超时后进行补偿，并且不会锁定整个资源。

##### 数据一致性

引入补偿机制，由业务活动管理器控制一致性。

#### Try阶段

主要是对「业务系统做检测及资源预留」，其主要分为两个阶段。

#### Confirm阶段

主要是对「业务系统做确认提交」，Try阶段执行成功并开始执行 Confirm阶段时，默认 Confirm阶段是不会出错的。即：只要Try成功，Confirm一定成功。

#### Cancel阶段

主要是在业务执行错误，需要回滚的状态下执行的业务取消，「预留资源释放」

<img src="./images/SYS301013.png" />

## 消息队列相关事务

### 本地消息表

消息生产方建一个消息表记录消息发送状态。消息表和业务数据要在一个事务里提交，然后消息会经过MQ发送到消息的消费方，如果发送失败会进行重试。

消息消费方接收这个消息并完成自己的业务逻辑。

如果是业务的失败，可以给生产方发送一个业务补偿消息，通知生产方进行回滚等操作。

此时如果本地事务处理成功，表明已经处理成功了；如果处理失败，那么就会重试执行。

生产方和消费方定时扫描本地消息表，把还没处理完成的消息或者失败的消息再发送一遍。

<img src="./images/SYS301014.png" />

### 消息事务<img src="./icons/xiaohui.gif" />

消息事务的原理是将两个事务通过消息中间件进行异步解耦实现系统的最终一致性。和本地消息表有点类似，但是是通过消息中间件的机制去做的，其本质就是将本地消息表封装到了消息中间件中。

这个方式避免了像XA协议那样的性能问题。目前市面上实现该方案的只有阿里的 RocketMQ.

#### 执行流程

发送prepare消息到消息中间件，成功后执行本地事务。

- 如果事务执行成功，则commit，消息中间件将消息下发至消费端。

- 如果事务执行失败，则回滚，消息中间件将这条prepare消息删除。

消费端接收到消息进行消费，如果消费失败，则不断重试。

### 最大努力通知

最大努力通知的方案实现比较简单，适用于一些最终一致性要求较低的业务。

#### 执行流程

系统A本地事务执行完之后，发送消息到MQ；

这里会有个专门消费 MQ 的服务，这个服务会消费MQ并调用系统B的接口；

要是系统B执行成功就ok了；要是系统B执行失败了，那么最大努力通知服务就定时尝试重新调用系统B, 反复N次，最后还是不行就放弃。

## Paxos<img src="./icons/netease.gif" /><img src="./icons/paypal.gif" /><img src="./icons/alibaba.gif" /><img src="./icons/xiaohui.gif" /><img src="./icons/bytedance.gif" />

Paxos算法是一种基于消息传递且具有高度容错特性的一致性算法。Paxos 算法解决的问题是一个分布式系统如何就某个值（决议）达成一致。

一个典型的场景是，在一个分布式数据库系统中，如果各节点的初始状态一致，每个节点执行相同的操作序列，那么他们最后能得到一个一致的状态。为保证每个节点执行相同的命令序列，需要在每一条指令上执行一个“一致性算法”以保证每个节点看到的指令一致。zookeeper使用的ZAB算法是该算法的一个实现。

### 三种角色

- **Proposer** 只要Proposer发的提案被半数以上Acceptor接受，Proposer就认为该提案里的value被选定了。
- **Acceptor** 只要Acceptor接受了某个提案，Acceptor就认为该提案里的value被选定了。
- **Learner** Acceptor 告诉Learner哪个value被选定，Learner就认为那个value被选定。

## 两个阶段

- 阶段一 准leader确定
  - Proposer选择一个提案编号N，然后向半数以上的Acceptor发送编号为N的Prepare请求。
  - 如果一个Acceptor收到一个编号为N的Prepare请求，且N大于该Acceptor已经响应过的所有Prepare请求的编号，那么它就会将它已经接受过的编号最大的提案（如果有的话）作为响应反馈给Proposer，同时该Acceptor承诺不再接受任何编号小于N的提案。

- 阶段二 leader确认
  - 如果Proposer收到半数以上Acceptor对其发出的编号为N的Prepare请求的响应，那么它就会发送一个针对[N,V]提案的Accept请求给半数以上的Acceptor。注意：V就是收到的响应中编号最大的提案的value，如果响应中不包含任何提案，那么V就由Proposer自己决定。
  - 如果Acceptor收到一个针对编号为N的提案的Accept请求，只要该Acceptor没有对编号大于N的Prepare请求做出过响应，它就接受该提案。

<img src="./images/SYS301015.png" />

### 约束条件

#### 正确性

指只有一个提议值会生效。

因为 Paxos 协议要求每个生效的提议被多数 Acceptor 接收，并且 Acceptor 不会接受两个不同的提议，因此可以保证正确性。

#### 可终止性

指最后总会有一个提议生效。

Paxos 协议能够让 Proposer 发送的提议朝着能被大多数 Acceptor 接受的那个提议靠拢，因此能够保证可终止性。

## Raft<img src="./icons/xiaohui.gif" />

Raft算法是一种简单易懂的共识算法。它依靠状态机和主从同步的方式，在各个节点之间实现数据的一致性。

### 竞选主节点

有三种节点：

- Leader 主节点

- Follower 从节点

- Candidate 参与投票竞争的节点

Leader会周期性的发送心跳包给Follower. 每个Follower都设置了一个随机的竞选超时时间，一般为150ms~300ms，如果在这个时间内没有收到Leader的心跳包，就会变成Candidate，进入竞选阶段。

Candidate会发送投票请求给其他所有节点，如果收到超过一半节点的回复，Candidate就会晋升为Leader然后周期性地发送心跳包给Follower. Follower收到心跳包重新开始计时。

如果有多个Candidate票数打平，就需要重新开始投票。由于每个节点设置的随机竞选超时时间不同，因此下一次再次出现多个Candidate并获得同样票数的概率很低。

### 数据同步

来自客户端的修改都会被传入Leader写入日志，然后Leader会把修改复制到所有Follower, 等待半数以上Follower反馈成功写入日志以后，先自己提交修改，再通知所有Follower提交修改。

